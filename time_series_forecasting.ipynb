{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Forecasting with PyTorch Transformer network\n",
    "\n",
    "This notebook has been created for running the code on your own system.\n",
    "You will require a Nvidia GPU or TPU in order to run the training part of this project.\n",
    "\n",
    "This is based on the pipeline script (pipeline.sh & pipeline.ps1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate_time_serie.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from uuid import uuid4\n",
    "\n",
    "periods = [7, 14, 28, 30]\n",
    "\n",
    "\n",
    "def get_init_df():\n",
    "\n",
    "    date_rng = pd.date_range(start=\"2015-01-01\", end=\"2020-01-01\", freq=\"D\")\n",
    "\n",
    "    dataframe = pd.DataFrame(date_rng, columns=[\"timestamp\"])\n",
    "\n",
    "    dataframe[\"index\"] = range(dataframe.shape[0])\n",
    "\n",
    "    dataframe[\"article\"] = uuid4().hex\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def set_amplitude(dataframe):\n",
    "\n",
    "    max_step = random.randint(90, 365)\n",
    "    max_amplitude = random.uniform(0.1, 1)\n",
    "    offset = random.uniform(-1, 1)\n",
    "\n",
    "    phase = random.randint(-1000, 1000)\n",
    "\n",
    "    amplitude = (\n",
    "        dataframe[\"index\"]\n",
    "        .apply(lambda x: max_amplitude * (x % max_step + phase) / max_step + offset)\n",
    "        .values\n",
    "    )\n",
    "\n",
    "    if random.random() < 0.5:\n",
    "        amplitude = amplitude[::-1]\n",
    "\n",
    "    dataframe[\"amplitude\"] = amplitude\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def set_offset(dataframe):\n",
    "\n",
    "    max_step = random.randint(15, 45)\n",
    "    max_offset = random.uniform(-1, 1)\n",
    "    base_offset = random.uniform(-1, 1)\n",
    "\n",
    "    phase = random.randint(-1000, 1000)\n",
    "\n",
    "    offset = (\n",
    "        dataframe[\"index\"]\n",
    "        .apply(\n",
    "            lambda x: max_offset * np.cos(x * 2 * np.pi / max_step + phase)\n",
    "            + base_offset\n",
    "        )\n",
    "        .values\n",
    "    )\n",
    "\n",
    "    if random.random() < 0.5:\n",
    "        offset = offset[::-1]\n",
    "\n",
    "    dataframe[\"offset\"] = offset\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def generate_time_series(dataframe):\n",
    "\n",
    "    clip_val = random.uniform(0.3, 1)\n",
    "\n",
    "    period = random.choice(periods)\n",
    "\n",
    "    phase = random.randint(-1000, 1000)\n",
    "\n",
    "    dataframe[\"views\"] = dataframe.apply(\n",
    "        lambda x: np.clip(\n",
    "            np.cos(x[\"index\"] * 2 * np.pi / period + phase), -clip_val, clip_val\n",
    "        )\n",
    "        * x[\"amplitude\"]\n",
    "        + x[\"offset\"],\n",
    "        axis=1,\n",
    "    ) + np.random.normal(\n",
    "        0, dataframe[\"amplitude\"].abs().max() / 10, size=(dataframe.shape[0],)\n",
    "    )\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def generate_df():\n",
    "    dataframe = get_init_df()\n",
    "    dataframe = set_amplitude(dataframe)\n",
    "    dataframe = set_offset(dataframe)\n",
    "    dataframe = generate_time_series(dataframe)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('Room-climate.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run main method for generate_time_serie.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "for _ in tqdm(range(200)):\n",
    "    df = generate_df()\n",
    "\n",
    "    # fig = plt.figure()\n",
    "    # plt.plot(df[-120:][\"index\"], df[-120:][\"views\"])\n",
    "    # plt.show()\n",
    "\n",
    "    dataframes.append(df)\n",
    "\n",
    "all_data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "all_data.to_csv(\"data/data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def add_date_cols(dataframe: pd.DataFrame, date_col: str = \"timestamp\"):\n",
    "    \"\"\"\n",
    "    add time features like month, week of the year ...\n",
    "    :param dataframe:\n",
    "    :param date_col:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    dataframe[date_col] = pd.to_datetime(dataframe[date_col], format=\"%Y-%m-%d\")\n",
    "\n",
    "    dataframe[\"day_of_month\"] = dataframe[date_col].dt.day / 31\n",
    "    dataframe[\"day_of_year\"] = dataframe[date_col].dt.dayofyear / 365\n",
    "    dataframe[\"month\"] = dataframe[date_col].dt.month / 12\n",
    "    dataframe[\"week_of_year\"] = dataframe[date_col].dt.isocalendar().week / 53\n",
    "    dataframe[\"year\"] = (dataframe[date_col].dt.year - 2015) / 5\n",
    "\n",
    "    return dataframe, [\"day_of_month\", \"day_of_year\", \"month\", \"week_of_year\", \"year\"]\n",
    "\n",
    "\n",
    "def add_basic_lag_features(\n",
    "    dataframe: pd.DataFrame,\n",
    "    group_by_cols: List,\n",
    "    col_names: List,\n",
    "    horizons: List,\n",
    "    fill_na=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes simple lag features\n",
    "    :param dataframe:\n",
    "    :param group_by_cols:\n",
    "    :param col_names:\n",
    "    :param horizons:\n",
    "    :param fill_na:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    group_by_data = dataframe.groupby(by=group_by_cols)\n",
    "\n",
    "    new_cols = []\n",
    "\n",
    "    for horizon in horizons:\n",
    "        dataframe[[a + \"_lag_%s\" % horizon for a in col_names]] = group_by_data[\n",
    "            col_names\n",
    "        ].shift(periods=horizon)\n",
    "        new_cols += [a + \"_lag_%s\" % horizon for a in col_names]\n",
    "\n",
    "    if fill_na:\n",
    "        dataframe[new_cols] = dataframe[new_cols].fillna(0)\n",
    "\n",
    "    return dataframe, new_cols\n",
    "\n",
    "\n",
    "def process_df(dataframe: pd.DataFrame, target_col: str = \"views\"):\n",
    "\n",
    "    \"\"\"\n",
    "    :param dataframe:\n",
    "    :param target_col:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    dataframe, new_cols = add_date_cols(dataframe, date_col=\"timestamp\")\n",
    "    dataframe, lag_cols = add_basic_lag_features(\n",
    "        dataframe, group_by_cols=[\"article\"], col_names=[target_col], horizons=[1]\n",
    "    )\n",
    "\n",
    "    return dataframe, new_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run main method from data_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/data.csv\")\n",
    "out_path = \"data/processed_data.csv\"\n",
    "config_path = \"data/config.json\"\n",
    "\n",
    "data, cols = process_df(data)\n",
    "\n",
    "data.to_csv(out_path, index=False)\n",
    "\n",
    "config = {\n",
    "    \"features\": cols,\n",
    "    \"target\": \"views\",\n",
    "    \"group_by_key\": \"article\",\n",
    "    \"lag_features\": [\"views_lag_1\"],\n",
    "}\n",
    "with open(config_path, \"w\") as f:\n",
    "    json.dump(config, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## time_series_forecasting/training.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from time_series_forecasting.model import TimeSeriesForcasting\n",
    "\n",
    "\n",
    "def split_df(\n",
    "    df: pd.DataFrame, split: str, history_size: int = 120, horizon_size: int = 30\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a training / validation samples\n",
    "    Validation samples are the last horizon_size rows\n",
    "\n",
    "    :param df:\n",
    "    :param split:\n",
    "    :param history_size:\n",
    "    :param horizon_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if split == \"train\":\n",
    "        end_index = random.randint(horizon_size + 1, df.shape[0] - horizon_size)\n",
    "    elif split in [\"val\", \"test\"]:\n",
    "        end_index = df.shape[0]\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    label_index = end_index - horizon_size\n",
    "    start_index = max(0, label_index - history_size)\n",
    "\n",
    "    history = df[start_index:label_index]\n",
    "    targets = df[label_index:end_index]\n",
    "\n",
    "    return history, targets\n",
    "\n",
    "\n",
    "def pad_arr(arr: np.ndarray, expected_size: int = 120):\n",
    "    \"\"\"\n",
    "    Pad top of array when there is not enough history\n",
    "    :param arr:\n",
    "    :param expected_size:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    arr = np.pad(arr, [(expected_size - arr.shape[0], 0), (0, 0)], mode=\"edge\")\n",
    "    return arr\n",
    "\n",
    "\n",
    "def df_to_np(df):\n",
    "    arr = np.array(df)\n",
    "    arr = pad_arr(arr)\n",
    "    return arr\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, groups, grp_by, split, features, target):\n",
    "        self.groups = groups\n",
    "        self.grp_by = grp_by\n",
    "        self.split = split\n",
    "        self.features = features\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.groups)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        group = self.groups[idx]\n",
    "\n",
    "        df = self.grp_by.get_group(group)\n",
    "\n",
    "        src, trg = split_df(df, split=self.split)\n",
    "\n",
    "        src = src[self.features + [self.target]]\n",
    "\n",
    "        src = df_to_np(src)\n",
    "\n",
    "        trg_in = trg[self.features + [f\"{self.target}_lag_1\"]]\n",
    "\n",
    "        trg_in = np.array(trg_in)\n",
    "        trg_out = np.array(trg[self.target])\n",
    "\n",
    "        src = torch.tensor(src, dtype=torch.float)\n",
    "        trg_in = torch.tensor(trg_in, dtype=torch.float)\n",
    "        trg_out = torch.tensor(trg_out, dtype=torch.float)\n",
    "\n",
    "        return src, trg_in, trg_out\n",
    "\n",
    "\n",
    "def train(\n",
    "    data_csv_path: str,\n",
    "    feature_target_names_path: str,\n",
    "    output_json_path: str,\n",
    "    log_dir: str = \"ts_logs\",\n",
    "    model_dir: str = \"ts_models\",\n",
    "    batch_size: int = 32,\n",
    "    epochs: int = 10,\n",
    "    horizon_size: int = 30,\n",
    "):\n",
    "    data = pd.read_csv(data_csv_path)\n",
    "\n",
    "    with open(feature_target_names_path) as f:\n",
    "        feature_target_names = json.load(f)\n",
    "\n",
    "    data_train = data[~data[feature_target_names[\"target\"]].isna()]\n",
    "\n",
    "    grp_by_train = data_train.groupby(by=feature_target_names[\"group_by_key\"])\n",
    "\n",
    "    groups = list(grp_by_train.groups)\n",
    "\n",
    "    full_groups = [\n",
    "        grp for grp in groups if grp_by_train.get_group(grp).shape[0] > 2 * horizon_size\n",
    "    ]\n",
    "\n",
    "    train_data = Dataset(\n",
    "        groups=full_groups,\n",
    "        grp_by=grp_by_train,\n",
    "        split=\"train\",\n",
    "        features=feature_target_names[\"features\"],\n",
    "        target=feature_target_names[\"target\"],\n",
    "    )\n",
    "    val_data = Dataset(\n",
    "        groups=full_groups,\n",
    "        grp_by=grp_by_train,\n",
    "        split=\"val\",\n",
    "        features=feature_target_names[\"features\"],\n",
    "        target=feature_target_names[\"target\"],\n",
    "    )\n",
    "\n",
    "    print(\"len(train_data)\", len(train_data))\n",
    "    print(\"len(val_data)\", len(val_data))\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=10,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=10,\n",
    "        shuffle=False,\n",
    "    )\n",
    "\n",
    "    model = TimeSeriesForcasting(\n",
    "        n_encoder_inputs=len(feature_target_names[\"features\"]) + 1,\n",
    "        n_decoder_inputs=len(feature_target_names[\"features\"]) + 1,\n",
    "        lr=1e-5,\n",
    "        dropout=0.1,\n",
    "    )\n",
    "\n",
    "    logger = TensorBoardLogger(\n",
    "        save_dir=log_dir,\n",
    "    )\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"valid_loss\",\n",
    "        mode=\"min\",\n",
    "        dirpath=model_dir,\n",
    "        filename=\"ts\",\n",
    "    )\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=epochs,\n",
    "        gpus=1,\n",
    "        logger=logger,\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    result_val = trainer.test(test_dataloaders=val_loader)\n",
    "\n",
    "    output_json = {\n",
    "        \"val_loss\": result_val[0][\"test_loss\"],\n",
    "        \"best_model_path\": checkpoint_callback.best_model_path,\n",
    "    }\n",
    "\n",
    "    if output_json_path is not None:\n",
    "        with open(output_json_path, \"w\") as f:\n",
    "            json.dump(output_json, f, indent=4)\n",
    "\n",
    "    return output_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training.py main method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv_path_p = \"data/processed_data.csv\"\n",
    "feature_target_names_path_p = \"data/config.json\"\n",
    "log_dir_p = \"models/ts_views_logs\"\n",
    "model_dir_p = \"models/ts_views_models\"\n",
    "output_json_path_p = \"models/trained_config.json\"\n",
    "\n",
    "train(\n",
    "    data_csv_path=data_csv_path_p,\n",
    "    feature_target_names_path=feature_target_names_path_p,\n",
    "    output_json_path=output_json_path_p,\n",
    "    log_dir=log_dir_p,\n",
    "    model_dir=model_dir_p\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "from time_series_forecasting.model import TimeSeriesForcasting\n",
    "from time_series_forecasting.training import split_df, Dataset\n",
    "\n",
    "\n",
    "def smape(true, pred):\n",
    "    \"\"\"\n",
    "    Symmetric mean absolute percentage error\n",
    "    :param true:\n",
    "    :param pred:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    true = np.array(true)\n",
    "    pred = np.array(pred)\n",
    "\n",
    "    smape_val = (\n",
    "        100\n",
    "        / pred.size\n",
    "        * np.sum(2 * (np.abs(true - pred)) / (np.abs(pred) + np.abs(true) + 1e-8))\n",
    "    )\n",
    "\n",
    "    return smape_val\n",
    "\n",
    "\n",
    "def evaluate_regression(true, pred):\n",
    "    \"\"\"\n",
    "    eval mae + smape\n",
    "    :param true:\n",
    "    :param pred:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    return {\"smape\": smape(true, pred), \"mae\": mean_absolute_error(true, pred)}\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    data_csv_path: str,\n",
    "    feature_target_names_path: str,\n",
    "    trained_json_path: str,\n",
    "    eval_json_path: str,\n",
    "    horizon_size: int = 30,\n",
    "    data_for_visualization_path: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the last 8 labeled weeks of the data.\n",
    "    Compares the model to a simple baseline : prediction the last known value\n",
    "    :param data_csv_path:\n",
    "    :param feature_target_names_path:\n",
    "    :param trained_json_path:\n",
    "    :param eval_json_path:\n",
    "    :param horizon_size:\n",
    "    :param data_for_visualization_path:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(data_csv_path)\n",
    "\n",
    "    with open(trained_json_path) as f:\n",
    "        model_json = json.load(f)\n",
    "\n",
    "    model_path = model_json[\"best_model_path\"]\n",
    "\n",
    "    with open(feature_target_names_path) as f:\n",
    "        feature_target_names = json.load(f)\n",
    "\n",
    "    target = feature_target_names[\"target\"]\n",
    "\n",
    "    data_train = data[~data[target].isna()]\n",
    "\n",
    "    grp_by_train = data_train.groupby(by=feature_target_names[\"group_by_key\"])\n",
    "\n",
    "    groups = list(grp_by_train.groups)\n",
    "\n",
    "    full_groups = [\n",
    "        grp for grp in groups if grp_by_train.get_group(grp).shape[0] > horizon_size\n",
    "    ]\n",
    "\n",
    "    val_data = Dataset(\n",
    "        groups=full_groups,\n",
    "        grp_by=grp_by_train,\n",
    "        split=\"val\",\n",
    "        features=feature_target_names[\"features\"],\n",
    "        target=feature_target_names[\"target\"],\n",
    "    )\n",
    "\n",
    "    model = TimeSeriesForcasting(\n",
    "        n_encoder_inputs=len(feature_target_names[\"features\"]) + 1,\n",
    "        n_decoder_inputs=len(feature_target_names[\"features\"]) + 1,\n",
    "        lr=1e-4,\n",
    "        dropout=0.5,\n",
    "    )\n",
    "    model.load_state_dict(torch.load(model_path)[\"state_dict\"])\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    gt = []\n",
    "    baseline_last_known_values = []\n",
    "    neural_predictions = []\n",
    "\n",
    "    data_for_visualization = []\n",
    "\n",
    "    for i, group in tqdm(enumerate(full_groups[:100])):\n",
    "        time_series_data = {\"history\": [], \"ground_truth\": [], \"prediction\": []}\n",
    "\n",
    "        df = grp_by_train.get_group(group)\n",
    "        src, trg = split_df(df, split=\"val\")\n",
    "\n",
    "        time_series_data[\"history\"] = src[target].tolist()[-120:]\n",
    "        time_series_data[\"ground_truth\"] = trg[target].tolist()\n",
    "\n",
    "        last_known_value = src[target].values[-1]\n",
    "\n",
    "        trg[\"last_known_value\"] = last_known_value\n",
    "\n",
    "        gt += trg[target].tolist()\n",
    "        baseline_last_known_values += trg[\"last_known_value\"].tolist()\n",
    "\n",
    "        src, trg_in, _ = val_data[i]\n",
    "\n",
    "        src, trg_in = src.unsqueeze(0), trg_in.unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction = model((src, trg_in[:, :1, :]))\n",
    "            for j in range(1, horizon_size):\n",
    "                last_prediction = prediction[0, -1]\n",
    "                trg_in[:, j, -1] = last_prediction\n",
    "                prediction = model((src, trg_in[:, : (j + 1), :]))\n",
    "\n",
    "            trg[target + \"_predicted\"] = (prediction.squeeze().numpy()).tolist()\n",
    "\n",
    "            neural_predictions += trg[target + \"_predicted\"].tolist()\n",
    "\n",
    "            time_series_data[\"prediction\"] = trg[target + \"_predicted\"].tolist()\n",
    "\n",
    "        data_for_visualization.append(time_series_data)\n",
    "\n",
    "    baseline_eval = evaluate_regression(gt, baseline_last_known_values)\n",
    "    model_eval = evaluate_regression(gt, neural_predictions)\n",
    "\n",
    "    eval_dict = {\n",
    "        \"Baseline_MAE\": baseline_eval[\"mae\"],\n",
    "        \"Baseline_SMAPE\": baseline_eval[\"smape\"],\n",
    "        \"Model_MAE\": model_eval[\"mae\"],\n",
    "        \"Model_SMAPE\": model_eval[\"smape\"],\n",
    "    }\n",
    "\n",
    "    if eval_json_path is not None:\n",
    "        with open(eval_json_path, \"w\") as f:\n",
    "            json.dump(eval_dict, f, indent=4)\n",
    "\n",
    "    if data_for_visualization_path is not None:\n",
    "        with open(data_for_visualization_path, \"w\") as f:\n",
    "            json.dump(data_for_visualization, f, indent=4)\n",
    "\n",
    "    for k, v in eval_dict.items():\n",
    "        print(k, v)\n",
    "\n",
    "    return eval_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run main method of evaluation.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv_path = \"data/processed_data.csv\"\n",
    "feature_target_names_path = \"data/config.json\"\n",
    "trained_json_path = \"models/trained_config.json\"\n",
    "eval_json_path = \"data/eval.json\"\n",
    "data_for_visualization_path = \"data/visualization.json\"\n",
    "\n",
    "evaluate(\n",
    "    data_csv_path=data_csv_path,\n",
    "    feature_target_names_path=feature_target_names_path,\n",
    "    trained_json_path=trained_json_path,\n",
    "    eval_json_path=eval_json_path,\n",
    "    data_for_visualization_path=data_for_visualization_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open(\"data/visualization.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "os.makedirs(\"data/images\", exist_ok=True)\n",
    "\n",
    "for i, sample in enumerate(data):\n",
    "    hist_size = len(sample[\"history\"])\n",
    "    gt_size = len(sample[\"ground_truth\"])\n",
    "    plt.figure()\n",
    "    plt.plot(range(hist_size), sample[\"history\"], label=\"History\")\n",
    "    plt.plot(\n",
    "        range(hist_size, hist_size + gt_size), sample[\"ground_truth\"], label=\"Ground Truth\"\n",
    "    )\n",
    "    plt.plot(\n",
    "        range(hist_size, hist_size + gt_size), sample[\"prediction\"], label=\"Prediction\"\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Time\")\n",
    "\n",
    "    plt.ylabel(\"Time Series\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.savefig(f\"data/images/{i}.png\")\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "43d91c62f5e04c931190bcff90b6028456b5c53954089b8df069d6337588d705"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('py38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
